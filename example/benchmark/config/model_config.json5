{
  model: {
    // model types: 'mopp', 'more', 'saber'
    model_name: 'td3_bc',
    // Parameters for each model
    // Imitation algorithms
    bc: {
      // set the model training schedule ('b': behavior, 'd': dynamics, 'q': Q-value function, 'vae_s': vae_s, 'agent': agent)
      train_schedule: ['agent'],
      hyper_params: {
        model_params: {p: [[300, 300],]},
        // p, entropy
        optimizers: {p: ['adam', 1e-5], entropy: ['adam', 1e-3]}
      },
    },
    td3_bc: {
      train_schedule: ['agent'],
      hyper_params: {
        model_params: {q: [[256, 256], 2], p: [[256, 256],]},
        optimizers: {q: ['adam', 3e-4], p: ['adam', 3e-4]}
      }
    },
    doge: {
      train_schedule: ['agent'],
      hyper_params: {
        model_params: {q: [[256, 256], 2], p: [[256, 256],], distance: [[256, 256, 256],]},
        optimizers: {q: ['adam', 3e-4], p: ['adam', 3e-4], distance:  ['adam', 1e-3]},
        alpha: 17.5,
        lambda_lr: 0.0003,
        initial_lambda: 5,
        train_d_steps: 100000,
        N: 20,
      }
    },
    h2o: {
      train_schedule: ['agent'],
      hyper_params: {
        model_params: {q: [[256, 256], 2], p: [[256, 256],], dsa: [[256],], dsas: [[256],]},
        optimizers: {q: ['adam', 3e-4], p: ['adam', 3e-4], dsa: ['adam', 3e-4], dsas: ['adam', 3e-4], alpha: ['adam', 3e-4], alpha_prime: ['adam', 3e-4]},
        alpha: 0.01,
        lambda_lr: 0.0003,
      }
    }
  },
  env: {
    basic_info: {state_dim: null, state_min: null, state_max: null,
      action_dim: null, action_min: null, action_max: null},
    // Parameters for Env which is provided externally
    external: {benchmark_name: 'd4rl', data_source: 'mujoco', env_name: 'Hopper-v2', data_name: 'hopper_random-v1',
    data_file_path: null, state_normalize: false, reward_normalize: false, num_transitions: -1,
    score_normalize: false, score_norm_min: null, score_norm_max: null},
    learned: {
      // 'mlp', 'prob', 'rnn', 'adm'
      dynamic_module_type: 'mlp',
      // If the dynamics predict the reward or not.
      with_reward: true,
      // parameters for each type of dynmamics models
      mlp: {
        model_params: [[200, 200], 3],
        optimizers: ['adam', 1e-3],
      },
    }
  },
  train: {
    device: 'cuda',
    data_loader_name: null,  // 'app' for real-world application data.
    train_test_ratio: 0.99,
    batch_size: 64,
    model_buffer_size: 1000000,
    weight_decays: 0.0,
    update_freq: 1,
    update_rate: 0.005,
    discount: 0.99,
    total_train_steps: 10000,
    summary_freq: 100,
    print_freq: 1000,
    save_freq: 10000,
    eval_freq: 5000,
    // parameters for model files
    model_dir: 'models',
    behavior_ckpt_name: 'b',
    dynamics_ckpt_name: 'd',
    q_ckpt_name: 'q',
    vae_s_ckpt_name: 'vae_s',
    agent_ckpt_name: 'agent',
    seed: 1,
    // parameters for Wandb logger
    wandb: {entity: null, project: null, name: null, reinit: false, mode: 'online'}
  },
  eval: {
    n_eval_episodes: 20,
    episode_step: 10,
    log_dir: 'eval',
    start: 0.,
    steps: 100,
  },
  interface: {
    policy_file: null,
    log_path: null,
  }
}