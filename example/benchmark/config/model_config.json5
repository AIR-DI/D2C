{
  model: {
    // model types: 'mopp', 'more', 'saber'
    model_name: 'td3_bc',
    // Parameters for each model
    // Imitation algorithms
    bc: {
      // set the model training schedule ('b': behavior, 'd': dynamics, 'q': Q-value function, 'vae_s': vae_s, 'agent': agent)
      train_schedule: ['agent'],
      hyper_params: {
        model_params: {p: [[300, 300],]},
        // p, entropy
        optimizers: {p: ['adam', 1e-5], entropy: ['adam', 1e-3]}
      },
    },
    td3_bc: {
      train_schedule: ['agent'],
      hyper_params: {
        model_params: {q: [[400, 400], 2], p: [[300, 300],]},
        optimizers: {q: ['adam', 1e-3], p: ['adam', 1e-5]}
      }
    }
  },
  env: {
    basic_info: {state_dim: null, action_dim: null},
    // Parameters for Env which is provided externally
    external: {benchmark_name: 'd4rl', data_source: 'mujoco', env_name: 'Hopper-v2', data_name: 'hopper_random-v1',
    data_file_path: null, state_normalize: false, reward_normalize: true, num_transitions: -1,
    score_normalize: false, score_norm_min: null, score_norm_max: null},
    learned: {
      // 'dnn', 'prob', 'rnn', 'adm'
      dynamic_module_type: 'prob',
      // If the dynamics predict the reward or not.
      with_reward: true,
      // parameters for each type of dynmamics models
      dnn: {
        model_params: [[200, 200], 3],
        optimizers: [['adam', 1e-3],],
      },
      prob: {
        model_params: [[200, 200, 200], 3],
        optimizers: [['adam', 1e-3],],
        mode: 'local',
      },
      rnn: {
        model_params: [[200, 200], 3],
        optimizers: [['adam', 1e-3],],
        warm_steps: 5,
        out_steps: 10,
        mode: 'local',
      },
      adm: {
        model_params: null,
        optimizers: null,
      },
    }
  },
  train: {
    device: 'cuda',
    data_loader_name: null,  // 'app' for real-world application data.
    train_test_ratio: 0.99,
    batch_size: 64,
    model_buffer_size: 1000000,
    weight_decays: 1e-5,
    update_freq: 1,
    update_rate: 0.005,
    discount: 0.99,
    total_train_steps: 10000,
    summary_freq: 100,
    print_freq: 1000,
    save_freq: 10000,
    eval_freq: 5000,
    // parameters for model files
    model_dir: 'models',
    behavior_ckpt_name: 'b',
    dynamics_ckpt_name: 'd',
    q_ckpt_name: 'q',
    vae_s_ckpt_name: 'vae_s',
    agent_ckpt_name: 'agent',
    seed: 1,
  },
  eval: {
    n_eval_episodes: 20,
    episode_step: 10,
    log_dir: 'eval',
    start: 0.,
    steps: 100,
  },
  interface: {
    policy_file: null,
    log_path: null,
  }
}