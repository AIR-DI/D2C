{
  model: {
    // model types: 'mopp', 'more', 'saber'
    model_name: 'td3_bc',
    // Parameters for each model
    // Imitation algorithms
    bc: {
      // set the model training schedule ('b': behavior, 'd': dynamics, 'q': Q-value function, 'vae_s': vae_s, 'agent': agent)
      train_schedule: ['agent'],
      hyper_params: {
        model_params: {p: [[256, 256],]},
        // p
        optimizers: {p: ['adam', 3e-4],},
        test_data_ratio: 0.1,
        test_freq: 100,
      },
    },
    td3_bc: {
      train_schedule: ['agent'],
      hyper_params: {
        model_params: {q: [[256, 256], 2], p: [[256, 256],]},
        optimizers: {q: ['adam', 3e-4], p: ['adam', 3e-4]}
      }
    },
    doge: {
      train_schedule: ['agent'],
      hyper_params: {
        model_params: {q: [[256, 256], 2], p: [[256, 256],], distance: [[256, 256, 256],]},
        optimizers: {q: ['adam', 3e-4], p: ['adam', 3e-4], distance:  ['adam', 1e-3]},
        alpha: 17.5,
        lambda_lr: 0.0003,
        initial_lambda: 5,
        train_d_steps: 100000,
        N: 20,
      }
    },
    dmil: {
      train_schedule: ['agent'],
      hyper_params: {
        model_params: {f: [[256, 256],], p: [[256, 256],], d:[[512, 512],]},
        optimizers: {f: ['adam', 1e-4], p: ['adam', 3e-4], d: ['adam', 1e-4]},
        rollout_size: null,
      }
    },

  },
  env: {
    basic_info: {state_dim: null, state_min: null, state_max: null,
      action_dim: null, action_min: null, action_max: null},
    // Parameters for Env which is provided externally
    external: {},
    learned: {
      // 'mlp', 'prob', 'rnn', 'adm'
      dynamic_module_type: 'mlp',
      // If the dynamics predict the reward or not.
      with_reward: true,
      // parameters for each type of dynmamics models
      mlp: {
        model_params: [[200, 200], 3],
        optimizers: ['adam', 1e-3],
      },
    }
  },
  train: {
    device: 'cuda',
    data_loader_name: 'app',  // 'app' for real-world application data.
    action_noise: null,  // The std value of the gaussian noise added to the action.
    data_split_ratio: null, // The ratio for splitting the dataset. It should be in (0, 1].
    test_data_ratio: 0.1,
    batch_size: 64,
    model_buffer_size: null,  // The capacity of the empty buffer.
    weight_decays: 0.0,
    update_freq: 1,
    update_rate: 0.005,
    discount: 0.99,
    total_train_steps: 10000,
    summary_freq: 100,
    print_freq: 1000,
    save_freq: 10000,
    eval_freq: 10000,
    // parameters for model files
    model_dir: 'models',
    behavior_ckpt_name: 'b',
    dynamics_ckpt_name: 'd',
    q_ckpt_name: 'q',
    vae_s_ckpt_name: 'vae_s',
    agent_ckpt_name: 'agent',
    seed: 1,
    // parameters for Wandb logger
    wandb: {entity: null, project: null, name: null, reinit: false, mode: 'online'}
  },
  eval: {
    n_eval_episodes: 10,
    episode_step: 10,
    log_dir: 'eval',
    start: 0.,
    steps: 100,
  },
  interface: {
    policy_file: null,
    log_path: null,
  }
}